{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ri5cVDAWIgp7",
        "MqR6Klwc-UAH",
        "A6gy4MLGIgp9",
        "32vFPqeB-UAJ",
        "B9hosQNsPiw0",
        "_9FW41E0WuWg",
        "IdPMtwGPqLDK"
      ],
      "mount_file_id": "1MsIwZL9GB6DnXojABJF5N2V1zIPRxx3d",
      "authorship_tag": "ABX9TyMKT61nrxA2Yb2s8xEQX1BR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thippawan72/pythainlp/blob/dev/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VhBw2AmK4sGQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "jEVHz8jCFOnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install moviepy"
      ],
      "metadata": {
        "id": "m23WOv5CmOHg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f05dnQR43St",
        "outputId": "d59ad5f6-a74e-41bd-df1a-d132f5371669"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip"
      ],
      "metadata": {
        "id": "e4GtPYPDnl3F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input and output directories\n",
        "formal_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/วิดีโอทั้งหมดกึ่งทางการ'\n",
        "formal_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ภาษาระดับกึ่งทางการ/Video_กึ่งทางการ'\n",
        "\n",
        "casual_input_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /วิดีโอทั้งหมดการใช้ชีวิตประจำวัน'\n",
        "casual_output_directory = '/content/drive/MyDrive/Thesis/วิดีโอภาษามือ/ระดับการใช้ในชีวิตประจำวัน /Video_การใช้ชีวิตประจำวัน'"
      ],
      "metadata": {
        "id": "dxA0Dw77n1ZC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert GIF to MP4\n",
        "def convert_gif_to_mp4(input_gif, output_mp4):\n",
        "    try:\n",
        "        # Load the GIF file\n",
        "        clip = VideoFileClip(input_gif)\n",
        "\n",
        "        # Write the video to MP4 format\n",
        "        clip.write_videofile(output_mp4, codec='libx264')\n",
        "        print(f\"Successfully converted {input_gif} to {output_mp4}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {input_gif}: {e}\" )"
      ],
      "metadata": {
        "id": "1OSXPthpm8r3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all GIFs in a directory\n",
        "#def preprocess_videos(input_dir, output_dir):\n",
        "#   if not os.path.exists(output_dir):\n",
        " #       os.makedirs(output_dir)\n",
        "\n",
        "#    # Loop over all files in the input directory\n",
        " #   for filename in os.listdir(input_dir):\n",
        "  #      if filename.endswith(\".gif\"):\n",
        "   #         input_path = os.path.join(input_dir, filename)\n",
        "    #        output_filename = filename.replace(\".gif\", \".mp4\")\n",
        "     #       output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "            # Convert GIF to MP4\n",
        "      #      convert_gif_to_mp4(input_path, output_path)\n",
        "\n",
        "# Start preprocessing the GIFs\n",
        "#preprocess_videos(formal_input_directory, formal_output_directory)"
      ],
      "metadata": {
        "id": "HVpZAsKSna4v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5cVDAWIgp7"
      },
      "source": [
        "# PyThaiNLP Get Started\n",
        "\n",
        "Code examples for basic functions in PyThaiNLP https://github.com/PyThaiNLP/pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3HsfhZlwInqs"
      },
      "outputs": [],
      "source": [
        "# # pip install required modules\n",
        "# # uncomment if running from colab\n",
        "# # see list of modules in `requirements` and `extras`\n",
        "# # in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
        "\n",
        "#!pip install pythainlp\n",
        "#!pip install epitran"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install required modules"
      ],
      "metadata": {
        "id": "BaT_g8fV-7xp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp\n",
        "!pip install epitran"
      ],
      "metadata": {
        "id": "E32blbWe_CLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a7dd15-ff74-4d01-9dad-1d10c9f169c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.8.30)\n",
            "Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.4\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.9.11)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n",
            "Collecting jamo (from epitran)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting g2pk (from epitran)\n",
            "  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n",
            "Collecting konlpy (from g2pk->epitran)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting python-mecab-ko (from g2pk->epitran)\n",
            "  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n",
            "Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n",
            "Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=f22238c1d85bfd3f8b1ec6ec8936f487efdda0d917534486e438ec101356ebcc\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n",
            "Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR6Klwc-UAH"
      },
      "source": [
        "## Import PyThaiNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp"
      ],
      "metadata": {
        "id": "9IhnIXQJ-oDg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R7CkITTf-UAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93b4e667-306e-4ce9-cb24-a1da601104d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5.0.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6gy4MLGIgp9"
      },
      "source": [
        "## Thai Characters\n",
        "\n",
        "PyThaiNLP provides some ready-to-use Thai character set (e.g. Thai consonants, vowels, tonemarks, symbols) as a string for convenience. There are also few utility functions to test if a string is in Thai or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GAvoeZg3Igp-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7f5b3ddb-596f-4404-ae33-bd87c14a1f4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮฤฦะัาำิีึืุูเแโใไๅํ็่้๊๋ฯฺๆ์ํ๎๏๚๛๐๑๒๓๔๕๖๗๘๙฿'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "pythainlp.thai_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TFPtK_FL-UAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a9978d-0f1c-4596-c9c5-287665960402"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(pythainlp.thai_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uPwx53A6IgqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fb80b550-eeeb-4cca-f3c3-22910db9e59b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "pythainlp.thai_consonants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e5-lZjsd-UAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047a3b06-442b-4237-d997-43d2c29972bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(pythainlp.thai_consonants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5UA7Hwy_IgqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1831c301-39aa-451e-8937-d0061deb2c78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32vFPqeB-UAJ"
      },
      "source": [
        "## Checking if a string contains Thai character or not, or how many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "t3NvXqYFIgqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd1e5ee-3000-4f33-e85d-d450a1bcddb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import pythainlp.util\n",
        "\n",
        "pythainlp.util.isthai(\"ก\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sRzSQjugIgqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58636361-682c-4dcb-b2ea-da0f0ed4a44f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DP5yfJebIgqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a61e29fd-6eeb-4973-c56f-9f0d0c9c6611"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\", ignore_chars=\".()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ภาษาระดับกึ่งทางการ\n"
      ],
      "metadata": {
        "id": "FxkLG5Av-BoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "from pythainlp.tokenize import subword_tokenize\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.corpus import thai_stopwords"
      ],
      "metadata": {
        "id": "WDZtB1BND8m_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category มหาวิทยาลัย\n",
        "```\n",
        "1.สวัสดีค่ะอาจารย์ที่ปรึกษา\n",
        "2.นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\n",
        "3.ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\n",
        "4.นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\n",
        "5.ผมขอสอบถามเกี่ยวกับการเตรียมพร้อมในการสอบวิชาอาจาร์ยหน่อยครับ\n",
        "6.นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\n",
        "7.นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\n",
        "8.คุณคือหัวหน้า เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\n",
        "9.นักศึกษาหมดกำลังใจในการทำงาน\n",
        "10.อาจาร์ยที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\n",
        "```"
      ],
      "metadata": {
        "id": "PN8cnBkOV-Ul"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFPOHyZIgqh"
      },
      "source": [
        "###Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5P_YygrIgqm"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIXUxXlTIgqo"
      },
      "source": [
        "Default word tokenizer use a word list from `pythainlp.corpus.common.thai_words()`.\n",
        "We can get that list, add/remove words, and create new tokenizer from the modified list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SklPJ-DbIgqi"
      },
      "source": [
        "### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "words = set(thai_words())\n",
        "words.add(\"อาจารย์\")\n",
        "words.add(\"ปรึกษา\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text1))"
      ],
      "metadata": {
        "id": "IKvEDM3U-YGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35697213-1e73-4dc0-9bd0-d0f06585a3de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JEbY-MGCIgqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcb580d-fc2a-49e4-afb3-d2cbcb4ccb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ],
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text2))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text3))"
      ],
      "metadata": {
        "id": "Dc9PVhShepRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d69af2b-e234-481c-f888-0bec6497b5a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text4))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text4))"
      ],
      "metadata": {
        "id": "D7EBXclqYM6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1317699e-3b10-413b-ca4b-54cebc348639"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "custom dictionary: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text5))\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYqQnvnZ1V7P",
        "outputId": "aa30d012-150d-49e7-dfb2-928c255ee22b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text6))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14EMK8Db1WDU",
        "outputId": "c33f473e-2f55-44c1-ce7f-b8049e0fe1d8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่ง', 'สัปดาห์', 'หน้า']\n",
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text7))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqHgG3eo1i5C",
        "outputId": "4e04b65e-b74e-4faa-be84-ec330f1acdd0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การเรียน', 'ได้']\n",
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text8))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzV1lNl2RvQw",
        "outputId": "b69fecc0-5182-4a50-9194-24c272df9dea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text9))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPY9kXuJZGCM",
        "outputId": "91f7a13e-7eb3-4eb3-fa6d-df8bc677f736"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การทำงาน']\n",
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text10))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8W4aQntZGiM",
        "outputId": "979caa71-0119-4601-edb6-24b0299c236b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['อาจารย์ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อ', 'หัว', 'นี้']\n",
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp.tokenize import word_detokenize\n",
        "print(word_detokenize(['โน็', 'ตบุ๊ค']))"
      ],
      "metadata": {
        "id": "p7Q3qoEPYegy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "407b1573-e7b5-4c8b-a540-89dc9a3f9517"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "โน็ตบุ๊ค\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Word Removal\n"
      ],
      "metadata": {
        "id": "lRuhGEg-O53A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ตัดคำ\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text1))\n",
        "words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "# ดึงรายการ stop words ในภาษาไทย\n",
        "stopwords = thai_stopwords()\n",
        "# แปลง stop words เป็นเซ็ต (set) สำหรับการเพิ่ม/ลบ\n",
        "\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_8keLiHlENu",
        "outputId": "ff54b981-37a3-49f3-e383-7676efce2a1b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text2))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text2))\n",
        "words = custom_tokenizer.word_tokenize(text2)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "FrLPcE7cO53B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3cf980d-4826-4608-8044-ca4af3cd9550"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำแนะนำ', 'ได้', 'กับ', 'อาจารย์ที่ปรึกษา']\n",
            "custom dictionary : ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ที่ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text3))\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text3))\n",
        "words = custom_tokenizer.word_tokenize(text3)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c497kmcx91yE",
        "outputId": "e0af8795-debd-4092-fef9-cbcd89d1a70a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำแนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "custom dictionary : ['ขอ', 'ขอ', 'บคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยว', 'กับ', 'การเลือกวิชาเรียน']\n",
            "Original words: ['ขอ', 'ขอ', 'บคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยว', 'กับ', 'การเลือกวิชาเรียน']\n",
            "Filtered words: ['บคุณ', 'อาจารย์', 'คำ', 'แนะนำ', 'เกี่ยว', 'การเลือกวิชาเรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text4))\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text4))\n",
        "words = custom_tokenizer.word_tokenize(text4)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDO0LL50HIbB",
        "outputId": "0a54191e-7ac0-441a-bb02-0d7eef1e734f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ต้องการ', 'โน', '็ต', 'บุ๊ค', 'ใน', 'การทำงาน', 'หรือไม่']\n",
            "custom dictionary : ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Original words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Filtered words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text5))\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text5))\n",
        "words = custom_tokenizer.word_tokenize(text5)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "0A45r2s7_aI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a403d40-85c6-47f0-a85a-08cafc55023a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Original words: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Filtered words: ['ผม', 'สอบถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text6))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text6))\n",
        "words = custom_tokenizer.word_tokenize(text6)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKpgpjKnQJE-",
        "outputId": "53e6bd60-2f15-4220-d5cb-66be6aec80b6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่ง', 'สัปดาห์', 'หน้า']\n",
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Original words: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Filtered words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text7))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text7))\n",
        "words = custom_tokenizer.word_tokenize(text7)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKyVCPIIQJhc",
        "outputId": "7e0bc203-f2f8-441b-b38b-514b25a12e24"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การเรียน', 'ได้']\n",
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text8))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))\n",
        "words = custom_tokenizer.word_tokenize(text8)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"คุณ\")\n",
        "stopwords.discard(\"เขียน\")\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.add(\" \")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a4uthukQJ-g",
        "outputId": "1b891c44-5f28-40bf-d2a4-e3376b97f56a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Original words: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Filtered words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text9))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text9))\n",
        "words = custom_tokenizer.word_tokenize(text9)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_20r-hDQZsx",
        "outputId": "97b863e5-2ae8-401f-bbd2-ef00711e6bbb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การทำงาน']\n",
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Original words: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Filtered words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text10))\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text10))\n",
        "print(\"custom dictionary:\", custom_tokenizer.word_tokenize(text8))\n",
        "words = custom_tokenizer.word_tokenize(text10)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.discard(\"ตาม\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfu-82g3QaFc",
        "outputId": "06225476-5aea-4424-c2e3-e85caac9a76e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['อาจารย์ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อ', 'หัว', 'นี้']\n",
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ', '-', 'นามสกุล', 'นักศึกษา', 'ที่', 'ลา', 'ออกมา', 'ให้', 'อาจารย์']\n",
            "Original words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Filtered words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech tagging"
      ],
      "metadata": {
        "id": "B9hosQNsPiw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ZNUCg1s3Piw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7118bf8d-22fe-4e5d-c74e-ae4d893b40f7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('สวัสดี', 'NCMN'), ('อาจารย์', 'NCMN'), ('ที่ปรึกษา', 'NCMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_tsl_grammar(words):\n",
        "    # ตัวอย่างกฎพื้นฐานสำหรับ TSL\n",
        "    words = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
        "    pos_tag(words)\n",
        "\n",
        "    expected_order = ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา'] # รูปแบบที่คาดหวัง\n",
        "    tags = pos_tag(words)  # รับ POS tags สำหรับคำ\n",
        "    print(\"POS Tags:\", tags)\n",
        "\n",
        "    # ตรวจสอบการเรียงลำดับพื้นฐานตามกฎที่กำหนดเอง\n",
        "    for i, word in enumerate(words):\n",
        "        if word in ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']:  # คำบอกเวลา\n",
        "            if i != 0:  # คำบอกเวลาควรอยู่ที่ตำแหน่งแรก\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# ใช้ฟังก์ชันเพื่อตรวจสอบ\n",
        "is_correct_order = check_tsl_grammar(words)\n",
        "print(\"Is correct order:\", is_correct_order)"
      ],
      "metadata": {
        "id": "8FqIUNUURcDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e37b3b0-7dfa-464b-c831-f2515982c085"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('สวัสดี', 'NCMN'), ('อาจารย์', 'NCMN'), ('ที่ปรึกษา', 'NCMN')]\n",
            "Is correct order: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "oLM9FnsfPiw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "\n",
        "print(get_lemma(\"ที่ปรึกษา\"))"
      ],
      "metadata": {
        "id": "E-x2DMApPiw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e000a8bb-474b-4282-8748-0eeb53ccf9bc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ปรึกษา\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# สร้าง custom dictionary\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "# ใช้ custom tokenizer\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "-SzUZngcPiw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ca6cc4-c603-4407-bc1d-6edefe52725e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['สวัสดี', 'ค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['สวัสดี', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['สวัสดี', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"นักศึกษาสามารถติดต่อขอคำแนะนำได้กับอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text2)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX0cV3pF867c",
        "outputId": "83b3abf2-3e2b-4496-90d4-be0630028ab9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'ขอ', 'คำ', 'แนะนำ', 'ได้', 'กับ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['นักศึกษา', 'สามารถ', 'ติดต่อ', 'คำ', 'แนะนำ', 'ได้', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"ขอขอบคุณอาจารย์สำหรับคำแนะนำเกี่ยวกับการเลือกวิชาเรียน\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.discard(\"คำแนะนำ\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text3)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึง stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYFnDY5h9wGp",
        "outputId": "4b7375da-b2ac-4575-b744-0dd2c722e6f1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Original words: ['ขอ', 'ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เกี่ยวกับ', 'การ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Filtered words: ['ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เลือก', 'วิชา', 'เรียน']\n",
            "Lemmatized words: ['ขอบคุณ', 'อาจารย์', 'สำหรับ', 'คำ', 'แนะนำ', 'เลือก', 'วิชา', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"นักศึกษาต้องการโน็ตบุ๊คในการทำงานหรือไม่\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"โน็ตบุ๊ค\")\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text4)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RVKekLnHDZt",
        "outputId": "75ca85a0-1b25-437d-eaab-7b50c6dd9ce4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Original words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ใน', 'การ', 'ทำงาน', 'หรือไม่']\n",
            "Filtered words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n",
            "Lemmatized words: ['นักศึกษา', 'ต้องการ', 'โน็ตบุ๊ค', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"ผมขอสอบถามเกี่ยวกับการสอบวิชาอาจารย์หน่อยครับ\"\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สอบถาม\": \"ถาม\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text5)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = thai_stopwords()\n",
        "stopwords = set(stopwords)\n",
        "stopwords.discard(\"เกี่ยวกับ\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzrsaAqeNI4G",
        "outputId": "3c2fad3e-0668-439c-c857-f041d023721e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Original words: ['ผม', 'ขอ', 'สอบถาม', 'เกี่ยวกับ', 'การ', 'สอบ', 'วิชา', 'อาจารย์', 'หน่อย', 'ครับ']\n",
            "Filtered words: ['ผม', 'สอบถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n",
            "Lemmatized words: ['ผม', 'ถาม', 'เกี่ยวกับ', 'สอบ', 'วิชา', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text6)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q48HssPL5Uk9",
        "outputId": "b0598dc1-92e7-432d-d586-db9244d7c287"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Original words: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Filtered words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n",
            "Lemmatized words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text7 = \"นักศึกษาสามารถใช้อินเตอร์เน็ตในการเรียนได้\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การเรียน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text7)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaDoCQvM56XS",
        "outputId": "d0cf04f5-314d-4e67-e3fa-da6986ba4965"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Original words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'ใน', 'การ', 'เรียน', 'ได้']\n",
            "Filtered words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n",
            "Lemmatized words: ['นักศึกษา', 'สามารถ', 'ใช้', 'อินเตอร์เน็ต', 'เรียน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text8 = \"คุณคือหัวหน้าใช่มั้ย เขียนชื่อ-นามสกุลนักศึกษาที่ลาออกมาให้อาจารย์\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ชื่อ-นามสกุล\")\n",
        "words.add(\"ลาออก\")\n",
        "words.discard(\"ออกมา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text8)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"คุณ\")\n",
        "stopwords.discard(\"เขียน\")\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.add(\" \")\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_6HG19H56uM",
        "outputId": "5442c329-3daf-4e67-b56a-1c8ccb3a0fff"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Original words: ['คุณ', 'คือ', 'หัวหน้า', 'ใช่', 'มั้ย', ' ', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ที่', 'ลาออก', 'มา', 'ให้', 'อาจารย์']\n",
            "Filtered words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n",
            "Lemmatized words: ['คุณ', 'หัวหน้า', 'เขียน', 'ชื่อ-นามสกุล', 'นักศึกษา', 'ลาออก', 'ให้', 'อาจารย์']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text9 = \"นักศึกษาหมดกำลังใจในการทำงาน\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.discard(\"การทำงาน\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text9)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"สามารถ\")\n",
        "stopwords.discard(\"คำ\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6_BznJO-IdZ",
        "outputId": "b64891ce-e9f7-47fb-fc2f-70b9a043cc66"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Original words: ['นักศึกษา', 'หมดกำลังใจ', 'ใน', 'การ', 'ทำงาน']\n",
            "Filtered words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n",
            "Lemmatized words: ['นักศึกษา', 'หมดกำลังใจ', 'ทำงาน']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "          \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "words = set(thai_words())\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text10)\n",
        "\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.discard(\"ตาม\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzkU_dVH-K2B",
        "outputId": "a3e972e0-d59f-48cc-e7b8-ff41b029a75a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Original words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Filtered words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n",
            "Lemmatized words: ['อาจารย์', 'ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "amtvFL9BPiw9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Xh7q8Ryy7it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สอบถาม\": \"ถาม\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# ฟังก์ชันการเรียงตามกฎไวยากรณ์ TSL\n",
        "def reorder_to_tsl(words):\n",
        "    # ลบคำที่ไม่จำเป็น เช่น คำบอกเพศ คำเสริม\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # เรียงลำดับโดยย้ายคำบอกเวลาไปด้านหน้า\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # รายการคำบอกเวลา\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # เรียงลำดับ (Time-Topic-Comment)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # รวมคำกลับเป็นประโยค\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "text10 = \"อาจารย์ที่ปรึกษาอนุมัติให้ทำงานตามข้อหัวนี้\"\n",
        "\n",
        "# เพิ่มคำพิเศษใน custom dictionary\n",
        "words = set(thai_words())  # thai_words() คืนค่า frozenset\n",
        "words.add(\"ข้อหัว\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "\n",
        "# ตัดคำด้วย custom dictionary\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text10)\n",
        "print(\"custom dictionary:\", tokenized_words)\n",
        "\n",
        "# ดึงรายการ stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "stopwords.discard(\"ให้\")\n",
        "stopwords.discard(\"ตาม\")\n",
        "stopwords.discard(\"ได้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in tokenized_words if word not in stopwords]\n",
        "print(\"Original words:\", tokenized_words)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้ฟังก์ชัน lemmatization\n",
        "lemmatized_words = [get_lemma(word) for word in filtered_words]\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n",
        "\n",
        "# เรียงลำดับตามกฎ TSL\n",
        "tsl_sentence = reorder_to_tsl(lemmatized_words)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P0FxFxxxZY5",
        "outputId": "2ea2d4b1-ced4-4fe3-b015-2d0b1b516768"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom dictionary: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Original words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว', 'นี้']\n",
            "Filtered words: ['อาจารย์', 'ที่ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n",
            "Lemmatized words: ['อาจารย์', 'ปรึกษา', 'อนุมัติ', 'ให้', 'ทำงาน', 'ตาม', 'ข้อหัว']\n",
            "Reordered sentence (TSL): อาจารย์ ปรึกษา อนุมัติ ให้ ทำงาน ตาม ข้อหัว\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl(words):\n",
        "    # ลบคำที่ไม่จำเป็น เช่น คำบอกเพศ คำเสริม\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # เรียงลำดับโดยย้ายคำบอกเวลาไปด้านหน้า\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น', 'หนึ่งสัปดาห์หน้า']  # รายการคำบอกเวลา\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # เรียงลำดับ (Time-Topic-Comment)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # รวมคำกลับเป็นประโยค\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ประโยคตัวอย่าง\n",
        "text6 = \"นักศึกษาควรเตรียมพร้อมสำหรับการสอบในหนึ่งสัปดาห์หน้า\"\n",
        "\n",
        "# เพิ่มคำพิเศษใน custom dictionary\n",
        "words = set(thai_words())  # thai_words() คืนค่า frozenset\n",
        "words.add(\"หนึ่งสัปดาห์หน้า\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "# ตัดคำด้วย custom dictionary\n",
        "words_tokenized = custom_tokenizer.word_tokenize(text6)\n",
        "\n",
        "# ดึงรายการ stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "# เพิ่มคำใหม่เข้าไปใน stop words\n",
        "stopwords.add(\"สำหรับ\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words_tokenized if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words_tokenized)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้คำที่กรองแล้วมาเรียงลำดับตามกฎ TSL\n",
        "tsl_sentence = reorder_to_tsl(filtered_words)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8YlmID4MGOR",
        "outputId": "a2bc05d3-918f-4bd3-d4b2-4e0a3d2e56c8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['นักศึกษา', 'ควร', 'เตรียมพร้อม', 'สำหรับ', 'การ', 'สอบ', 'ใน', 'หนึ่งสัปดาห์หน้า']\n",
            "Filtered words: ['นักศึกษา', 'เตรียมพร้อม', 'สอบ', 'หนึ่งสัปดาห์หน้า']\n",
            "Reordered sentence (TSL): หนึ่งสัปดาห์หน้า นักศึกษา เตรียมพร้อม สอบ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWJ9ivBtMGwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azdbUgZbMG1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUeRBzOiaIVZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Category ที่ทำงาน\n",
        "```\n",
        "11.เคยใช้กูเกิ้ลโครมเพื่อแชร์หรือไฟล์ในกูเกิ้ลไดร์ฟไหม\n",
        "12.ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\n",
        "13.ขอยืมเมาส์บลูทูธและแป้นพิมพ์ของเธอได้มั้ย\n",
        "14.ทักทายเพื่อนที่เป็นสมาชิกใหม่ในองค์กรคมนาคม\n",
        "15.ประสบการณ์การใช้หูฟังไร้สายช่วยลดความเพลียได้\n",
        "16.เห็นด้วยกับระเบียบใหม่ที่ให้อนุมัติได้ง่ายขึ้น\n",
        "17.อย่าดูถูกคนที่มีอายุน้อยและประสบการณ์ที่น้อยกว่า\n",
        "18.หมดแรงเพราะหน้าที่นี้มีการทำงานหนักมาก\n",
        "19.ถ้าคุณแนบเอกสารในกูเกิ้ลไดร์ฟแล้ว ช่วยตอบกลับอีเมลด้วย\n",
        "20.สามารถเขียนเว็บไซต์จากโปรแกรมคอมพิวเตอร์\n",
        "```\n"
      ],
      "metadata": {
        "id": "uQ9hn2tjOXP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization"
      ],
      "metadata": {
        "id": "NTUccqMaOLde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text11 = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text11))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text11))"
      ],
      "metadata": {
        "id": "JnPHAqegLPPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fc08d9-a943-4825-b665-f5726d997d74"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "custom dictionary : ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text12 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text12))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfqrVQuJaiuN",
        "outputId": "2339d5e3-8ee0-4ad2-81ca-f45d50cf33e5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text13 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text13))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-s04Rbeb2Qh",
        "outputId": "4aa67ad9-78ad-482c-e0c7-ef131c1f8cc1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text14 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text14))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text14))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov9ngcgAb2pw",
        "outputId": "875c2c7a-62a3-4bc7-e137-04b656f967a9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text15 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text15))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItqPSom3b3C1",
        "outputId": "6ee3762c-2fe5-452b-e6fa-f040c03cadfa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text16 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text16))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text16))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ZyCtDIb-my",
        "outputId": "7dd3d1ce-555b-4520-e4e3-ab8a6e924c2f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text17 = \"ตอบกลับในแอปพลิเคชั่นไลน์เพื่ออนุมัติหน้าที่ที่เห็นด้วย\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text17))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text17))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9inXLkkXb_bM",
        "outputId": "ac6d4d96-2e2a-4e7c-eaeb-b141f22dfd9a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n",
            "custom dictionary : ['ตอบกลับ', 'ใน', 'แอปพลิเคชั่น', 'ไลน์', 'เพื่อ', 'อนุมัติ', 'หน้าที่', 'ที่', 'เห็นด้วย']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stop Word Removal\n"
      ],
      "metadata": {
        "id": "Tc9PTjMLXP7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "id": "dny6xk9fzX2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0439e1-cad7-4c58-e0f4-c4506d9e52f4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'นี่แหละ', 'ระยะๆ', 'พวก', 'เกี่ยวกัน', 'อาจเป็นด้วย', 'ละ', 'แต่อย่างใด', 'ทัน', 'ใช่', 'เร็วๆ', 'จัดตั้ง', 'พบ', 'ปรากฏว่า', 'ฝ่าย', 'ช่วงนั้น', 'เมื่อใด', 'วันนั้น', 'สุดๆ', 'วัน', 'ตลอดไป', 'ไหนๆ', 'สูง', 'นิดหน่อย', 'สืบเนื่อง', 'เพิ่มเติม', 'นำมา', 'เพียงพอ', 'ทั้งมวล', 'เช่นดังว่า', 'ได้ที่', 'เมื่อนี้', 'บางๆ', 'ราย', 'อื่น', 'ง่าย', 'ตลอดจน', 'จ๊ะ', 'ยังแต่', 'แห่ง', 'ทุกชิ้น', 'ซึ่งก็', 'หมดสิ้น', 'ฉะนี้', 'รือ', 'ไม่', 'ครั้งครา', 'นํา', 'เผื่อจะ', 'ก็แล้วแต่', 'คราวหนึ่ง', 'จวนจะ', 'เกิด', 'ทำไม', 'จด', 'ซะจน', 'สั้น', 'ต่างๆ', 'เห็นแก่', 'ยิ่งขึ้น', 'พึ่ง', 'จน', 'พวกเขา', 'จนตลอด', 'เขา', 'เป็นการ', 'ผิด', 'มักจะ', 'จากนี้', 'นู้น', 'ที่ซึ่ง', 'ขั้น', 'เหลือเกิน', 'จากนั้น', 'ได้แก่', 'ครั้งที่', 'มึง', 'เช่นก่อน', 'จำ', 'เสร็จ', 'ข้างเคียง', 'อันใด', 'แต่ก็', 'แห่งใด', 'พื้นๆ', 'ใน', 'จับ', 'กว้าง', 'ไฉน', 'ล่าสุด', 'เป็นแต่', 'เช่นนั้น', 'ทันใดนั้น', 'แบบ', 'ข้างบน', 'ทั้ง', 'พวกแก', 'เก็บ', 'เมื่อคราว', 'คิด', 'ใช่ไหม', 'เฉพาะ', 'อันที่', 'ซะจนถึง', 'เมื่อ', 'ทั้งเป็น', 'เท่ากัน', 'ในช่วง', 'แล้วเสร็จ', 'พวกกู', 'ขณะใดๆ', 'รวด', 'รวม', 'ว่า', 'แต่ถ้า', 'เช่นที่', 'พยายาม', 'ตามแต่', 'ช้าๆ', 'เผื่อที่', '\\ufeffๆ', 'ภายภาคหน้า', 'ข้างๆ', 'เพราะว่า', 'ยิ่งแล้ว', 'กว่า', 'เยอะๆ', 'ดั่งกับ', 'เนี่ยเอง', 'แห่งโน้น', 'ครัน', 'เสร็จกัน', 'ดังเคย', 'มัน', 'สุด', 'จัดหา', 'แสดง', 'จำเป็น', 'ใกล้', 'แต่ละ', 'ก่อนหน้า', 'เธอ', 'พวกนั้น', 'เล็กน้อย', 'ไป่', 'สูงๆ', 'แม้ว่า', 'จู่ๆ', 'ถึงแม้จะ', 'อาจ', 'เปลี่ยน', 'ยังงั้น', 'บาง', 'แล้วกัน', 'และ', 'ถือว่า', 'อย่างใด', 'ง่ายๆ', 'ช่วง', 'กระผม', 'มีแต่', 'จัดการ', 'ต่อกัน', 'ถึงเมื่อ', 'อย่างนี้', 'นั่นแหละ', 'ครบ', 'น้อยๆ', 'ขณะนี้', 'ค่ะ', 'แห่งนี้', 'นี่เอง', 'มั้ยเนี่ย', 'ดั่งเก่า', 'แต่จะ', 'อย่างน้อย', 'เมื่อไหร่', 'อันได้แก่', 'รวมทั้ง', 'ครั้งก่อน', 'พวกนี้', 'นัก', 'แท้', 'เชื่อมั่น', 'ไร', 'ก่อนๆ', 'เยอะ', 'ทุกที', 'จึง', 'กู', 'เช่น', 'ยอมรับ', 'สิ่งนั้น', 'เหตุไร', 'เพียงแต่', 'แต่ทว่า', 'จนแม้น', 'เล็ก', 'แท้จริง', 'ดังกล่าว', 'เป็นอันว่า', 'นี่ไง', 'ทุกอัน', 'เสมือนกับ', 'ทีเดียว', 'เมื่อครั้ง', 'จวบจน', 'นับ', 'กระทำ', 'แยะๆ', 'เห็น', 'อย่างยิ่ง', 'นับแต่', 'ซึ่งกันและกัน', 'บางที่', 'ดัง', 'ทั้งปวง', 'ค่อยๆ', 'เสมือนว่า', 'จัดงาน', 'ระหว่าง', 'บ่อยครั้ง', 'เสียจน', 'เป็นต้น', 'ขณะเดียวกัน', 'เช่นดังที่', 'เกิน', 'ยัง', 'ที่ได้', 'ทุกครา', 'มากกว่า', 'ขวางๆ', 'ความ', 'ย่อย', 'ครับ', 'เผื่อว่า', 'ภายหน้า', 'ล้วน', 'พวกคุณ', 'ค่อนมาทาง', 'ส่วนที่', 'เป็นอันๆ', 'ประการฉะนี้', 'มันๆ', 'นับจากนั้น', 'ตรง', 'จ้ะ', 'หลาย', 'ร่วมด้วย', 'จนบัดนี้', 'ที่แท้จริง', 'บ้าง', 'นั่นไง', 'จ๋า', 'ทั้งที', 'ทุกตัว', 'โต', 'ที่แท้', 'เสียก่อน', 'รวดเร็ว', 'ทุกเมื่อ', 'ใช้', 'แล้ว', 'จนเมื่อ', 'ยิ่งขึ้นไป', 'บางกว่า', 'ร่วมกัน', 'พวกโน้น', 'เน้น', 'คือ', 'ซึ่งก็คือ', 'เมื่อไร', 'บ่อยกว่า', 'ต่างหาก', 'บัดเดี๋ยวนี้', 'คล้ายกับ', 'นับตั้งแต่', 'มองว่า', 'หนอย', 'ถ้าหาก', 'มิฉะนั้น', 'ยังงี้', 'คำ', 'สมัยโน้น', 'ค่อยไปทาง', 'ออก', 'ในระหว่าง', 'เพราะ', 'เถอะ', 'จะได้', 'เฉยๆ', 'ทุกหน', 'นั้นไว', 'เห็นว่า', 'เหตุ', 'จรดกับ', 'เอง', 'แต่เพียง', 'นั่นเป็น', 'ขึ้น', 'จัง', 'เสียนั่นเอง', 'หมดกัน', 'นอกเหนือ', 'เสร็จสิ้น', 'อย่างไรก็ได้', 'หนอ', 'บ่อยๆ', 'ก็ตามแต่', 'พา', 'อันๆ', 'ก็ตาม', 'อนึ่ง', 'คราวนี้', 'ปฏิบัติ', 'เต็มไปหมด', 'ด้วยว่า', 'ที่', 'คราว', 'ก็ตามที', 'เราๆ', 'ยังคง', 'เป็นเพื่อ', 'สั้นๆ', 'เช่นนั้นเอง', 'ทํา', 'คราหนึ่ง', 'เปิดเผย', 'ส่ง', 'บ่อย', 'ให้มา', 'ทั้งที่', 'เสียนั่น', 'สำคัญ', 'แต่เมื่อ', 'ทั้งหมด', 'จวบกับ', 'นี้แหล่', 'แค่นั้น', 'อื่นๆ', 'ขณะ', 'ดังกับว่า', 'ขาด', 'พร้อมทั้ง', 'เพื่อ', 'อย่างไรก็', 'กันดีกว่า', 'เท่าไร', 'ฯลฯ', 'จึงจะ', 'ใคร่จะ', 'นี่นา', 'ได้มา', 'ที่ๆ', 'ทรง', 'คราไหน', 'ทั่ว', 'เห็นจะ', 'มี', 'ซะก่อน', 'แล้วแต่', 'อันละ', 'ซึ่งๆ', 'กันเอง', 'นาน', 'เป็นที่', 'ผิดๆ', 'พบว่า', 'เป็นเพียงว่า', 'เป็นๆ', 'หากแม้นว่า', 'ปิด', 'คล้าย', 'เพียงไร', 'แต่ไร', 'สิ่งใด', 'ช่วงแรก', 'ยอม', 'ภาคฯ', 'บอก', 'นำพา', 'นาง', 'กล่าว', 'เช่นเคย', 'เห็นควร', 'ยิ่งเมื่อ', 'ภายนอก', 'ช่วงระหว่าง', 'ค่อน', 'จนกว่า', 'รวมๆ', 'คุณๆ', 'ส่วนใหญ่', 'จวน', 'ยืนยัน', 'เกือบจะ', 'ส่วนด้อย', 'ประกอบ', 'เพียง', 'เขียน', 'ตามที่', 'พวกนู้น', 'ทั้งนั้น', 'ครั้งหนึ่ง', 'อยาก', 'ถูก', 'เกี่ยวๆ', 'กันและกัน', 'เกินๆ', 'ก่อนหน้านี้', 'ตลอดวัน', 'หลัง', 'ครานั้น', 'เสร็จแล้ว', 'สิ้นกาลนาน', 'ใครๆ', 'แห่งไหน', 'รือว่า', 'คราวที่', 'จึงเป็น', 'เกี่ยวข้อง', 'ครบถ้วน', 'จำพวก', 'รวมถึง', 'เสียแล้ว', 'ตั้ง', 'ช่วงนี้', 'ด้วย', 'อันที่จริง', 'พวกฉัน', 'นอกจากนี้', 'ได้รับ', 'อย่างนั้น', 'ทั้งนั้นเพราะ', 'มั๊ย', 'แต่ว่า', 'ต่าง', 'ถึงเมื่อใด', 'ด้วยที่', 'มาก', 'ผู้ใด', 'เรื่อยๆ', 'น่า', 'ตน', 'คล้ายกัน', 'ประการหนึ่ง', 'ทุกคราว', 'ทาง', 'พึง', 'ลง', 'นับแต่นี้', 'แก่', 'เกี่ยวเนื่อง', 'แต่ไหน', 'พอสม', 'ณ', 'กลุ่มๆ', 'เหตุนั้น', 'แม้', 'ด้าน', 'กระนั้น', 'กันนะ', 'ใหญ่โต', 'ครั้งไหน', 'ทุกครั้ง', 'ตลอดปี', 'ที่แห่งนั้น', 'ซึ่งกัน', 'ปรับ', 'สูงสุด', 'เมื่อเช้า', 'นั่น', 'คราวๆ', 'จากนี้ไป', 'ตลอดทั่วถึง', 'แค่เพียง', 'ซะจนกระทั่ง', 'อันไหน', 'เรื่อย', 'ทว่า', 'ขณะใด', 'เป็นอัน', 'แยะ', 'ภายใน', 'ยกให้', 'ข้างล่าง', 'ทุกคน', 'ทำๆ', 'ทำให้', 'ยืนยง', 'เต็มๆ', 'พอ', 'ที', 'แม้แต่', 'ยังไง', 'เช่นไร', 'ทุกสิ่ง', 'ยาก', 'ผล', 'สิ่งไหน', 'ร่วม', 'เสียนี่กระไร', 'หนึ่ง', 'อย่างเช่น', 'แต่', 'ก็ได้', 'มัก', 'ถูกต้อง', 'ใคร', 'หลังจาก', 'นั้น', 'ต้อง', 'เสียยิ่ง', 'กว้างๆ', 'เล่าว่า', 'ยาว', 'นะ', 'ทุกที่', 'มุ่ง', 'แห่งนั้น', 'ขณะนั้น', 'หน่อย', 'ครั้งละ', 'ตั้งแต่', 'สําหรับ', 'หารือ', 'เพื่อที่', 'เนี่ย', 'ขณะหนึ่ง', 'ระยะ', 'ครานี้', 'ไม่ว่า', 'มิใช่', 'ย่อม', 'ในที่', 'บางครา', 'นับแต่ที่', 'คราวใด', 'จง', 'ส่วนเกิน', 'ตลอด', 'จนกระทั่ง', 'ส่วน', 'ค่อนข้างจะ', 'นั้นๆ', 'เช่นที่ว่า', 'ด้วยเหตุว่า', 'กระทั่ง', 'เป็นเพราะว่า', 'พวกมึง', 'พร้อมกัน', 'นาย', 'ดั่ง', 'วันไหน', 'เช่นดัง', 'มากมาย', 'สิ้น', 'สามารถ', 'นี้', 'หรือไง', 'ส่วนนั้น', 'ทุกอย่าง', 'เช่นกัน', 'ตรงๆ', 'สมัยนี้', 'นู่น', 'สู่', 'พร้อม', 'จะ', 'ตลอดทั้ง', 'วันใด', 'ต่างก็', 'ใคร่', 'เหลือ', 'อย่างไรเสีย', 'ทีใด', 'ข้างต้น', 'แสดงว่า', 'นั่นเอง', 'จริงจัง', 'ข้า', 'ทั้งนี้', 'ๆ', 'คงอยู่', 'น้อยกว่า', 'เปิด', 'เนื่องจาก', 'เท่านี้', 'คง', 'พอจะ', 'ทั้งสิ้น', 'กำลังจะ', 'ฯล', 'เพื่อที่จะ', 'ตลอดมา', 'แก้ไข', 'ถูกๆ', 'นี้เอง', 'นอกเหนือจาก', 'เพื่อให้', 'หรือ', 'ที่นี้', 'ตามด้วย', 'หน', 'อย่างละ', 'อาจเป็น', 'โดย', 'เสียนี่', 'ภาค', 'นอกจาก', 'พอเหมาะ', 'เพียงเพื่อ', 'เช่นเมื่อ', 'พร้อมที่', 'พวกมัน', 'คราวหลัง', 'ก็จะ', 'ตลอดกาลนาน', 'ตามๆ', 'อย่างที่', 'เข้า', 'รวมกัน', 'เล็กๆ', 'หากแม้น', 'เร็ว', 'เมื่อก่อน', 'บัดนั้น', 'ใหญ่ๆ', 'เสียจนกระทั่ง', 'บอกแล้ว', 'ตนเอง', 'ภายใต้', 'จนขณะนี้', 'ผ่าน', 'อัน', 'รับรอง', 'คราวไหน', 'ยังจะ', 'ที่ใด', 'ทันที', 'จาก', 'การ', 'อะไร', 'ขอ', 'เช่นใด', 'ครั้งนั้น', 'ต่อ', 'ถึงแก่', 'ครั้งกระนั้น', 'ใหม่ๆ', 'ปัจจุบัน', 'แค่จะ', 'ช่วงก่อน', 'จริงๆจังๆ', 'อดีต', 'ใหม่', 'เช่นดังก่อน', 'เพราะฉะนั้น', 'ช่วงที่', 'ที่ละ', 'เคยๆ', 'อย่างดี', 'ใต้', 'เกือบ', 'คิดว่า', 'บน', 'ล้วนแต่', 'ตลอดทั่ว', 'เท่านั้น', 'ร่วมมือ', 'เพียงไหน', 'เป็นที่สุด', 'จรด', 'ของ', 'ก็คือ', 'ครั้งหลัง', 'ช่วย', 'ด้วยเพราะ', 'ฝ่ายใด', 'ถือ', 'ยิ่งใหญ่', 'ใกล้ๆ', 'บัดนี้', 'สูงส่ง', 'เกี่ยวกับ', 'อย่างไร', 'ยิ่ง', 'บางขณะ', 'คราวโน้น', 'อย่างหนึ่ง', 'กันดีไหม', 'สมัยก่อน', 'อย่างมาก', 'ตลอดระยะเวลา', 'ทุกวันนี้', 'ยิ่งจะ', 'จวบ', 'ข้าฯ', 'เหล่า', 'จัดแจง', 'อย่างโน้น', 'ดังกับ', 'คราวนั้น', 'ตลอดทั่วทั้ง', 'ทำไร', 'แค่ว่า', 'เดียว', 'ช่วงหน้า', 'ยังโง้น', 'กันไหม', 'เมื่อวันวาน', 'เพียงแค่', 'คราใด', 'กว้างขวาง', 'แรก', 'หรือไร', 'เป็นเพียง', 'เสียจนถึง', 'หรือไม่', 'มิ', 'ที่นั้น', 'ถึงบัดนั้น', 'แต่ที่', 'ให้แด่', 'กับ', 'เพียงเพราะ', 'ข้าพเจ้า', 'ทุกทาง', 'ช่วงถัดไป', 'ชาว', 'ฉะนั้น', 'ตลอดศก', 'อย่างไหน', 'เช่นดังเก่า', 'จนทั่ว', 'มิได้', 'ทั้งหลาย', 'แค่ไหน', 'พอๆ', 'ประมาณ', 'จนถึง', 'ที่ว่า', 'จ้า', 'อันเนื่องมาจาก', 'ประสบ', 'ไกล', 'เอา', 'ครั้งนี้', 'นานๆ', 'เรา', 'สูงกว่า', 'ผ่านๆ', 'พวกเธอ', 'คงจะ', 'ตลอดกาล', 'นำ', 'เกือบๆ', 'พอแล้ว', 'จวนเจียน', 'เท่าไหร่', 'นอกจากว่า', 'สิ่งนี้', 'บอกว่า', 'หมด', 'เฉย', 'ทําให้', 'นางสาว', 'ไม่ใช่', 'เชื่อถือ', 'ที่แล้ว', 'ภายภาค', 'เพิ่งจะ', 'ถึงแม้ว่า', 'แต่เดิม', 'ถึงอย่างไร', 'ฯ', 'พอกัน', 'เยอะแยะ', 'ค่อนข้าง', 'นักๆ', 'เป็นต้นมา', 'รึว่า', 'อย่าง', 'วันนี้', 'ทั้งคน', 'ถ้า', 'เป็นแต่เพียง', 'สิ่ง', 'โตๆ', 'ก็', 'ให้แก่', 'คล้ายว่า', 'ค่อย', 'เพียงใด', 'เป็นด้วย', 'ด้วยเช่นกัน', 'กำลัง', 'เป็นดัง', 'จัดทำ', 'ปรากฏ', 'ด้วยเหตุนั้น', 'ดั่งเคย', 'เสีย', 'เท่าที่', 'บางที', 'นี่แน่ะ', 'เป็นอาทิ', 'มั้ยนั่น', 'ใหญ่', 'เชื่อ', 'ที่ไหน', 'ตนฯ', 'เสร็จสมบูรณ์', 'ข้าง', 'เมื่อครั้งก่อน', 'หากว่า', 'น่าจะ', 'ดั่งกับว่า', 'นับแต่นั้น', 'คล้ายกันกับ', 'นิดๆ', 'พอเพียง', 'กัน', 'ให้ไป', 'ในเมื่อ', 'รึ', 'กลุ่มก้อน', 'เช่นเดียวกัน', 'เริ่ม', 'ให้ดี', 'ไว้', 'แม้นว่า', 'ก็แค่', 'เป็นที', 'เหล่านี้', 'ประการใด', 'ทีไร', 'มั้ย', 'ล้วนจน', 'ส่วนใด', 'แม้กระทั่ง', 'ครั้งคราว', 'ด้วยเหตุที่', 'บางคราว', 'แก', 'ครั้งๆ', 'เท่า', 'เชื่อว่า', 'เต็มไปด้วย', 'คะ', 'ก็ต่อเมื่อ', 'หาก', 'เมื่อคืน', 'เป็น', 'ไม่ค่อย', 'มั้ยล่ะ', 'เมื่อวาน', 'พร้อมเพียง', 'จริง', 'บางแห่ง', 'กำหนด', 'เสียยิ่งนัก', 'สบาย', 'จนแม้', 'ด้วยกัน', 'แค่', 'ทันทีทันใด', 'ขณะที่', 'ตลอดถึง', 'ซะ', 'ช้านาน', 'แต่นั้น', 'ด้วยเหมือนกัน', 'พอควร', 'เป็นอันมาก', 'ถึงเมื่อไร', 'สมัย', 'เสียด้วย', 'เมื่อเย็น', 'ช่วงๆ', 'ไง', 'ถึงแม้', 'ทุก', 'เพิ่ง', 'เป็นต้นไป', 'ไม่ค่อยจะ', 'เมื่อนั้น', 'ยิ่งนัก', 'น่ะ', 'กลุ่ม', 'ประการ', 'ด้วยประการฉะนี้', 'ซึ่ง', 'ที่สุด', 'เมื่อคราวที่', 'ถึงจะ', 'ที่จริง', 'หาความ', 'ด้วยเหตุนี้', 'ยก', 'ไม่เป็นไร', 'เปลี่ยนแปลง', 'คราวละ', 'เถิด', 'ขวาง', 'หากแม้', 'ผู้', 'คราวก่อน', 'ซึ่งได้แก่', 'เลย', 'สมัยนั้น', 'จังๆ', 'คราที่', 'ใดๆ', 'พอดี', 'ช่วงหลัง', 'จัดให้', 'ทั้งๆ', 'ครั้งหลังสุด', 'เช่นเดียวกับ', 'กลับ', 'ควร', 'เอ็ง', 'น้อย', 'เป็นเพราะ', 'แค่นี้', 'ทุกวัน', 'ภาย', 'เผื่อ', 'เดียวกัน', 'ด้วยเหตุเพราะ', 'เมื่อคราวก่อน', 'เพื่อว่า', 'ยืนยาว', 'กล่าวคือ', 'ส่วนมาก', 'หรือยัง', 'บัดดล', 'แต่ต้อง', 'ทั้งนั้นด้วย', 'ช้า', 'ทีละ', 'มั้ยนะ', 'รับ', 'อยู่', 'ดังเก่า', 'นี่', 'ยิ่งจน', 'อาจจะ', 'นับจากนี้', 'ภายหลัง', 'ครั้ง', 'ครั้งใด', 'คราวหน้า', 'ทีเถอะ', 'นอก', 'อีก', 'พอตัว', 'มุ่งหมาย', 'มา', 'พอที่', 'ช่วงท้าย', 'พวกกัน', 'ไป', 'ไหน', 'เรียก', 'เหตุผล', 'นอกจากนั้น', 'เท่ากับ', 'พร้อมกับ', 'อย่างเดียว', 'ส่วนน้อย', 'อย่างๆ', 'คุณ', 'ถ้าจะ', 'ฉัน', 'เพิ่ม', 'พร้อมด้วย', 'ไม่ค่อยเป็น', 'นอกจากที่', 'ถึงบัดนี้', 'ยิ่งกว่า', 'รวมด้วย', 'ยืนนาน', 'ก็ดี', 'เรียบ', 'เคย', 'เช่นที่เคย', 'หาใช่', 'เข้าใจ', 'เช่นนี้', 'จัด', 'ครา', 'แต่ก่อน', 'หรือเปล่า', 'เหล่านั้น', 'อันจะ', 'ส่วนดี', 'นิด', 'เฉกเช่น', 'จริงๆ', 'เหตุนี้', 'บางครั้ง', 'คล้ายกับว่า', 'ช่วงต่อไป', 'ไกลๆ', 'ครบครัน', 'ตลอดเวลา', 'พวกที่', 'ได้แต่', 'พวกท่าน', 'อันที่จะ', 'พอที', 'ยาวนาน', 'ทั้งตัว', 'มุ่งเน้น', 'นอกนั้น', 'เท่าใด', 'กันเถอะ', 'พูด', 'พอสมควร', 'ก่อน', 'ถึง', 'ทุกแห่ง', 'แหละ', 'มอง', 'ทีๆ', 'ทุกๆ'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "# ตัดคำ\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))\n",
        "words = custom_tokenizer.word_tokenize(text)\n",
        "\n",
        "# ดึงรายการ stop words ในภาษาไทย\n",
        "stopwords = thai_stopwords()\n",
        "# แปลง stop words เป็นเซ็ต (set) สำหรับการเพิ่ม/ลบ\n",
        "\n",
        "stopwords = set(stopwords)\n",
        "\n",
        "# เพิ่มคำใหม่เข้าไปใน stop words\n",
        "stopwords.add(\"ไหม\")\n",
        "stopwords.add(\"ทำงาน\")\n",
        "\n",
        "# ลบคำบางคำออกจาก stop words\n",
        "stopwords.discard(\"เคย\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words:\", filtered_words)"
      ],
      "metadata": {
        "id": "o9UOjfeQX9o2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96293417-14f7-4c30-bd7c-edae76b5010f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['เคย', 'ใช้', 'กูเกิ้ล', 'โครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ล', 'ไดร์', 'ฟ', 'ไหม']\n",
            "custom dictionary : ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Original words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Filtered words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'แชร์', 'รูปภาพ', 'กูเกิ้ลไดร์ฟ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech tagging"
      ],
      "metadata": {
        "id": "_9FW41E0WuWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "ernbPKbDOq2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf2d0ad-828d-467e-ded9-481540821a3f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('เคย', 'XVMM'),\n",
              " ('ใช้', 'VACT'),\n",
              " ('กูเกิ้ลโครม', 'NCMN'),\n",
              " ('เพื่อ', 'RPRE'),\n",
              " ('แชร์', 'VACT'),\n",
              " ('รูปภาพ', 'NCMN'),\n",
              " ('ใน', 'RPRE'),\n",
              " ('กูเกิ้ลไดร์ฟ', 'NCMN'),\n",
              " ('ไหม', 'NCMN')]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization: Reduce each word to its base form, depending on its POS tag."
      ],
      "metadata": {
        "id": "vG3y24f52B2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The reordering of sentences based on Indian Sign Language grammar rules."
      ],
      "metadata": {
        "id": "IdPMtwGPqLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl(words):\n",
        "    # ลบคำที่ไม่จำเป็น เช่น คำบอกเพศ คำเสริม\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # เรียงลำดับโดยย้ายคำบอกเวลาไปด้านหน้า\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # รายการคำบอกเวลา\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # เรียงลำดับ (Time-Topic-Comment)\n",
        "    reordered_sentence = time_elements + non_time_elements\n",
        "\n",
        "    # รวมคำกลับเป็นประโยค\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ประโยคตัวอย่าง\n",
        "text = \"เคยใช้กูเกิ้ลโครมเพื่อแชร์รูปภาพในกูเกิ้ลไดร์ฟไหม\"\n",
        "\n",
        "# เพิ่มคำพิเศษใน custom dictionary\n",
        "words = set(thai_words())  # thai_words() คืนค่า frozenset\n",
        "words.add(\"กูเกิ้ลโครม\")\n",
        "words.add(\"กูเกิ้ลไดร์ฟ\")\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "\n",
        "# ตัดคำด้วย custom dictionary\n",
        "words_tokenized = custom_tokenizer.word_tokenize(text)\n",
        "\n",
        "# ดึงรายการ stop words ภาษาไทย\n",
        "stopwords = set(thai_stopwords())\n",
        "\n",
        "# เพิ่มคำใหม่เข้าไปใน stop words\n",
        "stopwords.add(\"ไหม\")\n",
        "stopwords.add(\"ทำงาน\")\n",
        "\n",
        "# ลบคำบางคำออกจาก stop words\n",
        "stopwords.discard(\"เคย\")\n",
        "stopwords.discard(\"ใช้\")\n",
        "\n",
        "# ลบคำที่อยู่ใน stop words\n",
        "filtered_words = [word for word in words_tokenized if word not in stopwords]\n",
        "\n",
        "print(\"Original words:\", words_tokenized)\n",
        "print(\"Filtered words:\", filtered_words)\n",
        "\n",
        "# ใช้คำที่กรองแล้วมาเรียงลำดับตามกฎ TSL\n",
        "tsl_sentence = reorder_to_tsl(filtered_words)\n",
        "print(\"Reordered sentence (TSL):\", tsl_sentence)"
      ],
      "metadata": {
        "id": "qoAZxOzaEPXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528be36a-345e-4294-a289-ad82f5a3aff8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'เพื่อ', 'แชร์', 'รูปภาพ', 'ใน', 'กูเกิ้ลไดร์ฟ', 'ไหม']\n",
            "Filtered words: ['เคย', 'ใช้', 'กูเกิ้ลโครม', 'แชร์', 'รูปภาพ', 'กูเกิ้ลไดร์ฟ']\n",
            "Reordered sentence (TSL): เคย ใช้ กูเกิ้ลโครม แชร์ รูปภาพ กูเกิ้ลไดร์ฟ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Combine and remove duplicates\n",
        "    reordered_sentence = list(dict.fromkeys(time_elements + nouns + others + verbs))\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "1_YvPxmnyLuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34666c61-bca1-4387-e066-dc64b23b237a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา: พรุ่งนี้ ตลาด ฉัน ไป\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "\n",
        "print(get_lemma(\"ที่ปรึกษา\"))"
      ],
      "metadata": {
        "id": "1w-R7DtYCsF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb51971c-cb26-4109-88a5-8ee67c301a35"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ปรึกษา\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"สวัสดีค่ะอาจารย์ที่ปรึกษา\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text1))\n",
        "\n",
        "# ฟังก์ชันสำหรับการปรับรูปเป็นคำพื้นฐาน\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",   # จับคู่ \"สวัสดีค่ะ\" เป็น \"สวัสดี\"\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",    # จับคู่ \"ที่ปรึกษา\" เป็น \"ปรึกษา\"\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# สร้าง custom dictionary\n",
        "words = set(thai_words())\n",
        "words.add(\"อาจารย์\")\n",
        "words.add(\"ปรึกษา\")\n",
        "words.discard(\"อาจารย์ที่ปรึกษา\")\n",
        "words.add(\"สวัสดีค่ะ\")  # เพิ่มคำ \"สวัสดีค่ะ\"\n",
        "\n",
        "# ใช้ custom tokenizer\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "tokenized_words = custom_tokenizer.word_tokenize(text1)\n",
        "\n",
        "# ใช้ฟังก์ชัน get_lemma เพื่อปรับคำ\n",
        "lemmatized_words = [get_lemma(word) for word in tokenized_words]\n",
        "\n",
        "print(\"newmm (custom dictionary):\", tokenized_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "GwDSJ7YXKKif",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "595d07fb-d7d2-492d-98ee-36077f2abea5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['สวัสดี', 'ค่ะ', 'อาจารย์ที่ปรึกษา']\n",
            "newmm (custom dictionary): ['สวัสดีค่ะ', 'อาจารย์', 'ที่ปรึกษา']\n",
            "Lemmatized words: ['สวัสดี', 'อาจารย์', 'ปรึกษา']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "from pythainlp.tag import pos_tag\n",
        "\n",
        "# ฟังก์ชันการหาคำพื้นฐาน (lemma)\n",
        "def get_lemma(word):\n",
        "    lemmas = {\n",
        "        \"ที่ปรึกษา\": \"ปรึกษา\",\n",
        "        \"สวัสดีค่ะ\": \"สวัสดี\",\n",
        "    }\n",
        "    return lemmas.get(word, word)\n",
        "\n",
        "# ฟังก์ชันการเรียงตามไวยากรณ์ TSL และเรียงคำนามก่อนกริยา พร้อมการใช้ lemma\n",
        "def reorder_to_tsl_and_noun_before_verb(sentence):\n",
        "    # Tokenize the sentence using PyThaiNLP\n",
        "    words = pythainlp.word_tokenize(sentence, engine='newmm')  # Using the 'newmm' tokenizer\n",
        "\n",
        "    # Simplify the sentence by removing articles and auxiliary verbs\n",
        "    simplified_sentence = [word for word in words if word not in ['คือ', 'เป็น', 'อยู่', 'มี', 'จะ', 'ได้', 'แล้ว', 'ก็', 'ที่', 'นั้น', 'นี้']]\n",
        "\n",
        "    # Step 1: Move time words to the beginning (TSL structure)\n",
        "    time_words = ['วันนี้', 'พรุ่งนี้', 'เมื่อวาน', 'ตอนเช้า', 'ตอนเย็น']  # List of common time-related words\n",
        "    time_elements = [word for word in simplified_sentence if word in time_words]\n",
        "    non_time_elements = [word for word in simplified_sentence if word not in time_words]\n",
        "\n",
        "    # Reorder the sentence (Time-Topic-Comment structure)\n",
        "    sentence_after_tsl = time_elements + non_time_elements\n",
        "\n",
        "    # Step 2: Perform part-of-speech tagging to identify nouns and verbs\n",
        "    pos_tags = pos_tag(sentence_after_tsl, engine='perceptron', corpus='orchid')\n",
        "\n",
        "    # Apply the custom get_lemma function to all words in the sentence\n",
        "    sentence_after_lemma = [get_lemma(word) for word in sentence_after_tsl]\n",
        "\n",
        "    # Separate nouns and verbs\n",
        "    nouns = [word for word, pos in pos_tags if pos.startswith('N')]  # Noun\n",
        "    verbs = [word for word, pos in pos_tags if pos.startswith('V')]  # Verb\n",
        "    others = [word for word, pos in pos_tags if not (pos.startswith('N') or pos.startswith('V'))]  # Other words\n",
        "\n",
        "    # Reorder: Nouns -> Others -> Verbs\n",
        "    reordered_sentence = time_elements + nouns + others + verbs\n",
        "\n",
        "    # Join words back into a string\n",
        "    return ' '.join(reordered_sentence)\n",
        "\n",
        "# ตัวอย่างประโยคภาษาไทย\n",
        "sentence3 = \"ฉันจะไปตลาดพรุ่งนี้\"\n",
        "\n",
        "# Reorder the sentence following TSL grammar and nouns before verbs\n",
        "reordered_sentence3 = reorder_to_tsl_and_noun_before_verb(sentence3)\n",
        "print(\"ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma:\", reordered_sentence3)"
      ],
      "metadata": {
        "id": "MLZ07UzPEC8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80468daf-8293-42df-9e22-b20eb49a3f41"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ประโยคที่เรียงตามไวยากรณ์ TSL และคำนามก่อนกริยา พร้อมการใช้ lemma: พรุ่งนี้ ตลาด พรุ่งนี้ ฉัน ไป\n"
          ]
        }
      ]
    }
  ]
}